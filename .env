# =============================================================================
# LLM Notetaker Environment Configuration
# =============================================================================

# =========================
# Compose & Template Settings
# =========================

# Token limits for different actions (higher = more detailed output)
RECIPE_MAX_TOKENS=2500          # Tokens for recipe templates (detailed recipes)
TEMPLATE_MAX_TOKENS=1500        # Tokens for general templates
GENERATE_MAX_TOKENS=1200        # Tokens for content generation
COMPOSE_MAX_TOKENS=800          # Tokens for other compose actions (simplify, improve, etc.)

# Model selection
COMPOSE_MODEL=llama3.2:3b       # Main model for compose actions
RECIPE_MODEL=gpt-oss:latest        # Specific model for recipe generation (can be larger)
AGENT_MODEL=gpt-oss:latest         # Model for agent interactions

# Input text limits
COMPOSE_MAX_CHARS=8000          # Maximum characters for input text
COMPOSE_CONCURRENCY=2           # Maximum parallel compose operations

# =========================
# Database Settings
# =========================

# Database path (optional override)
# DATABASE_PATH=instance/notetaker.db

# =========================
# Audio & TTS Settings
# =========================

# Whisper model for audio transcription
WHISPER_MODEL=medium            # Options: tiny, base, small, medium, large, large-v2, large-v3

# =========================
# Development Settings
# =========================

# Flask debug mode
# FLASK_DEBUG=true
# FLASK_ENV=development

# =========================
# Ollama Connection
# =========================

# Ollama API URL (if not running on localhost)
# OLLAMA_URL=http://127.0.0.1:11434

# =========================
# Performance Tuning
# =========================

# For very detailed recipes, you can increase these further:
# RECIPE_MAX_TOKENS=3000
# RECIPE_MODEL=llama3.1:8b

# For faster responses with smaller models:
# COMPOSE_MODEL=llama3.2:1b
# RECIPE_MODEL=llama3.2:1b

# =========================
# Usage Instructions
# =========================

# To use a larger model for better recipe quality:
# 1. Uncomment and set RECIPE_MODEL=llama3.1:8b (or codestral, mistral, etc.)
# 2. Increase RECIPE_MAX_TOKENS=3000 for very detailed recipes
# 3. Restart the application

# To optimize for speed:
# 1. Set COMPOSE_MODEL=llama3.2:1b
# 2. Reduce token limits (RECIPE_MAX_TOKENS=1500)
# 3. Increase COMPOSE_CONCURRENCY=4

# Example for high-quality recipe generation:
# RECIPE_MODEL=codestral:latest
# RECIPE_MAX_TOKENS=3500
# COMPOSE_MODEL=llama3.1:8b
